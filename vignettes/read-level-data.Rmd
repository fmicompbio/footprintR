---
title: "Exploring read-level footprinting data with `footprintR`"
output:
    rmarkdown::html_document:
        toc: true
        toc_float: true
vignette: >
  %\VignetteIndexEntry{Exploring read-level footprinting data with `footprintR`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Load data

We start the analysis by loading footprinting data at the level of individual
reads. Such data can be extracted from `modBam` files (standard bam file with
modification data stored in `MM` and `ML` tags, see
[SAMtags.pdf](https://samtools.github.io/hts-specs/SAMtags.pdf)) using
`readModBam()`. This is the preferred way described below.

Alternatively, read-level modification data can also be extracted from
`modBam` files using [modkit](https://nanoporetech.github.io/modkit) (see 
`modkitExtract()` for calling `modkit` from R and `readModkitExtract()`
to read the resulting output file). This will require a local installation of
`modkit` and may only be of interest if specific `modkit` features should
be used that are not available otherwise.

The `footprintR` package contains small example `modBam` files that were
generated using the [Dorado](https://github.com/nanoporetech/dorado) aligner:

```{r data-files, message=FALSE}
# load packages
library(footprintR)
library(SummarizedExperiment)

# read-level 6mA data generated by 'dorado'
modbamfiles <- system.file("extdata",
                           c("6mA_1_10reads.bam", "6mA_2_10reads.bam"),
                            package = "footprintR")
names(modbamfiles) <- c("sample1", "sample2")
```

Modification data is read from these files using `readModBam()`:

```{r read-data}
se <- readModBam(bamfiles = modbamfiles,
                 regions = "chr1:6940000-7000000",
                 modbase = "a",
                 verbose = TRUE)
se
```
This will create a `RangedSummarizedExperiment` object with positions in rows
and individual reads in columns:

```{r row-column-data}
# rows are positions...
rowRanges(se)

# ... and columns are reads
colData(se)
```

As you can see in the `colData(se)`, reads are assigned to samples,
and the read quality score (`qscore`) has also been extracted. The
sample names are obtained from the input files (here `extractfiles`), or
if the files are not named will be automatically assigned (each file
corresponding to a separate sample).

## Explore assay data

The single assay `mod_prob` is a `SparseMatrix` with modification probabilities.

```{r mod-prob-assay}
assayNames(se)

m <- assay(se, "mod_prob")
m
```

This matrix does not store the zeros and thus uses much less memory compared
to a normal (dense) matrix. However, you have to be careful when interpreting
the values in that matrix, as they follow a specific convention:

**Important: ** The zeros correspond to unobserved read/position combinations,
while all values that are 'implicitly' called (with a modifictation
probability of less than 5%) are represented with a value of 0.02.

That means if you would be manually calculating for example the average
modification probability at a given position, taking the row mean would
not be correct, as it would include the zero values that correspond to
unobserved data:

```{r mod-prob-zeros}
# modification probabilities at position chr1:6928850:-
m["chr1:6928850:-", ]

# WRONG: take the mean of all values
mean(m["chr1:6928850:-", ])

# CORRECT: exclude the zeros
non_zero <- m["chr1:6928850:-", ] != 0
mean(m["chr1:6928850:-", non_zero])
```

This is however rarely needed, as there are convenience functions that
will exclude the unobserved (zero) values automatically for you. For
example, you can summarize the reads in each sample using `summarizeOverReads()`
(see next section).

## Summarize read-level data

Summarized data can be obtained from the read-level data by calculating
a per-position summary over the reads in each sample:

```{r summarise-read-level-to-collapsed}
se_summary <- summarizeOverReads(se, keep.reads = TRUE,
                                 statistics = c("Nmod", "Nvalid", "FracMod",
                                                "Pmod", "AvgConf"))
```

As discussed above, this will automatically exclude the non-observed (zero)
values from the data when calculating the modification probability at each
position (`Pmod` assay):

```{r summarized-pmod}
assay(se_summary, "Pmod")["chr1:6928850:-", ]
```

The summary statistics to calculate are selected using the `statistics` argument.
By default, `summarizeOverReads()` will count the number of modified (`Nmod`)
and total (`Nvalid`) reads at each position and sample, and calculate the
fraction of modified bases from the two (`FracMod`).

```{r default-statistics}
assay(se_summary, "Nmod")["chr1:6928850:-", ]
assay(se_summary, "Nvalid")["chr1:6928850:-", ]
assay(se_summary, "FracMod")["chr1:6928850:-", ]
```

In the above example, we in addition also calculate the average modification
probability (`Pmod`) and the average confidence of the modification calls
per position (`AvgConf`). As we have set `keep.reads = TRUE`,
we get in addition also the read-level assay from the input object
(`mod_prob`) in which the reads are grouped by sample:

```{r collapsed-assays}
# read-level data is retained in "mod_prob" assay
assayNames(se_summary)

# ... which groups the reads by sample
assay(se_summary, "mod_prob")

# this means that the number of columns in "se_summary"
# is lower than in "se" (the number of samples instead
# of the number of reads)
dim(se)
dim(se_summary)

# the same is true for the assay "mod_prob" in "se" and
# "se_summary", however all read level data is still
# there and you can convert the assay back to ungrouped
# reads using as.matrix()
dim(assay(se, "mod_prob"))
dim(assay(se_summary, "mod_prob"))
dim(as.matrix(assay(se_summary, "mod_prob")))
```

## Plot data

The collapsed data can then be visualized just like the summary-level data
using `plotRegion`, using the `tracks.reads` argument instead of `tracks.summary`
to select the type of plot(s).

For reference, here we plot the summary-level data:

```{r plot-summary, fig.width=6, fig.height=4}
plotRegion(se_summary, region = "chr1:6932700-6932800",
           tracks.summary = list(FracMod = "Point"))
```

... and here we plot the read-level data of the same region:

```{r plot-region, warning=FALSE, fig.width=8, fig.height=2.5}
plotRegion(se, region = "chr1:6932700-6932800",
           tracks.summary = NULL,
           tracks.reads = list(mod_prob = "Heatmap"))

plotRegion(se, region = "chr1:6932700-6932800",
           tracks.summary = NULL,
           tracks.reads = list(mod_prob = "Lollipop"))
```

The x-axis in these plots is in "base-space", meaning that it shows the
coordinates of genomic bases on which modifications can be irregularly spaced.
Alternatively, we can also generate these plot in "modbase-space", in which
only modified bases are shown and the gaps between them are removed:

```{r plot-region-modbasespace, warning=FALSE, fig.width=8, fig.height=2.5}
plotRegion(se, region = "chr1:6932700-6932800",
           tracks.summary = NULL,
           tracks.reads = list(mod_prob = "Heatmap"),
           modbaseSpace = TRUE)

plotRegion(se, region = "chr1:6932700-6932800",
           tracks.summary = NULL,
           tracks.reads = list(mod_prob = "Lollipop"),
           modbaseSpace = TRUE)
```

As mentioned above, the read-level data is still contained in the
summary object `se_summary`, so we can plot both summary-level
and read-level data simultaneously with this object as input:

```{r plot-summary-and-read-level, fig.width=8, fig.height=6}
plotRegion(se_summary, region = "chr1:6932700-6932800",
           tracks.summary = list(FracMod = c("Smooth")),
           tracks.reads = list(mod_prob = c("Heatmap", "Lollipop")))
```

## Session info

```{r session-info}
sessionInfo()
```

