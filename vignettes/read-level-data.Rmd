---
title: "Exploring read-level footprinting data with `footprintR`"
output:
    rmarkdown::html_document:
        toc: true
        toc_float: true
vignette: >
  %\VignetteIndexEntry{Exploring read-level footprinting data with `footprintR`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Load data

We start the analysis by loading footprinting data at the level of individual
reads (for an example of working with summary-level data, see
`vignette("using-footprintR")`). Read-level data can be extracted from `modBam`
files (standard bam file with modification data stored in `MM` and `ML` tags,
see [SAMtags.pdf](https://samtools.github.io/hts-specs/SAMtags.pdf)) using
`readModBam()`. This is the preferred way described below.

Alternatively, read-level modification data can also be extracted from
`modBam` files using [modkit](https://nanoporetech.github.io/modkit) (see 
`modkitExtract()` for calling `modkit` from R and `readModkitExtract()`
to read the resulting output file). This will require a local installation of
`modkit` and may only be of interest if specific `modkit` features should
be used that are not available otherwise.

The `footprintR` package contains small example `modBam` files that were
generated using the [Dorado](https://github.com/nanoporetech/dorado) aligner:

```{r data-files, message=FALSE}
# load packages
library(footprintR)
library(SummarizedExperiment)

# read-level 6mA data generated by 'dorado'
modbamfiles <- system.file("extdata",
                           c("6mA_1_10reads.bam", "6mA_2_10reads.bam"),
                            package = "footprintR")
names(modbamfiles) <- c("sample1", "sample2")
```

Modification data is read from these files using `readModBam()`:

```{r read-data}
se <- readModBam(bamfiles = modbamfiles,
                 regions = "chr1:6940000-7000000",
                 modbase = "a",
                 verbose = TRUE)
se
```
This will create a `RangedSummarizedExperiment` object with positions in rows:

```{r row-data}
# rows are positions...
rowRanges(se)
```

Just like with summary-level data, columns correspond to samples
```{r column-data}
# ... and columns are samples
colData(se)
```
The sample names are obtained from the input files (here `extractfiles`), or
if the files are not named will be automatically assigned (each file
corresponding to a separate sample).

However, as each sample typically contains several reads (see `se$n_reads`),
the `qscore` column that stores the quality scores for individual reads, is
not a simple vector, but a list of vectors, in which scores are grouped by
sample. It may be needed to `unlist()` it if a flat vector is needed:
```{r coldata-list-columns}
se$qscore
unlist(se$qscore)
```

## Explore assay data

The single assay `mod_prob` is a `DataFrame` with modification probabilities.

```{r mod-prob-assay}
assayNames(se)

m <- assay(se, "mod_prob")
m
```
Again, to store read-level data for variable numbers of reads in each sample,
each column (sample) is not just a simple vector, but a position-by-read
`SparseMatrix`. If a simple matrix is needed, in which columns correspond to
reads instead of sample, it can be created using `as.matrix()`:
```{r mod-prob-columns}
m$sample1

as.matrix(m)
```

One advantage of the this grouping of reads per sample is that you can easily
perform per-sample operations using `lapply` (returns a `list`) or `endoapply`
(returns a `DataFrame`):
```{r mod-prob-lapply}
lapply(m, ncol)
endoapply(m, ncol)
```
The `SparseMatrix` objects do not store the zeros and thus use much less
memory compared to a normal (dense) matrix. However, you have to be careful when
interpreting the values in that matrix, as they follow a specific convention:

**Important: ** The zeros correspond to unobserved read/position combinations,
while all values that are 'implicitly' called (with a modifictation
probability of less than 5%) are represented with a value of 0.02.

That means if you would be manually calculating for example the average
modification probability at a given position, taking the row mean would
not be correct, as it would include the zero values that correspond to
unobserved data:

```{r mod-prob-zeros}
# modification probabilities at position chr1:6928850:-
m["chr1:6928850:-", ]

# WRONG: take the mean of all values
lapply(m["chr1:6928850:-", ], rowMeans)

# CORRECT: exclude the zeros
lapply(m["chr1:6928850:-", ], function(x) {
    non_zero <- SparseArray::nzwhich(x != 0)
    mean(x[non_zero])
})
```
This is however rarely needed, as there are convenience functions that
will exclude the unobserved (zero) values automatically for you. For
example, you can summarize the reads in each sample using `addReadsSummary()`
(see next section).

## Summarize read-level data

Summarized data can be obtained from the read-level data by calculating
a per-position summary over the reads in each sample:

```{r summarise-read-level}
se_summary <- addReadsSummary(se, keep.reads = TRUE,
                              statistics = c("Nmod", "Nvalid", "FracMod",
                                             "Pmod", "AvgConf"))
```

As discussed above, this will automatically exclude the non-observed (zero)
values from the data when calculating the modification probability at each
position (`Pmod` assay):

```{r summarized-pmod}
assay(se_summary, "Pmod")["chr1:6928850:-", ]
```

The summary statistics to calculate are selected using the `statistics` argument.
By default, `addReadsSummary()` will count the number of modified (`Nmod`)
and total (`Nvalid`) reads at each position and sample, and calculate the
fraction of modified bases from the two (`FracMod`).

```{r default-statistics}
assay(se_summary, "Nmod")["chr1:6928850:-", ]
assay(se_summary, "Nvalid")["chr1:6928850:-", ]
assay(se_summary, "FracMod")["chr1:6928850:-", ]
```

In the above example, we in addition also calculate the average modification
probability (`Pmod`) and the average confidence of the modification calls
per position (`AvgConf`). As we have set `keep.reads = TRUE`,
we get in addition also the read-level assay from the input object
(`mod_prob`) in which the reads are grouped by sample:

```{r collapsed-assays}
# read-level data is retained in "mod_prob" assay
assayNames(se_summary)

# ... which groups the reads by sample
assay(se_summary, "mod_prob")

# the dimensions of  read-level `se` and summarized `se_summary` are identical
dim(se)
dim(se_summary)
```

## Plot data

The read-level data can then be visualized just like the summary-level data
using `plotRegion`, using the `tracks.reads` argument instead of `tracks.summary`
to select the type of plot(s).

For reference, here we plot the summary-level data:

```{r plot-summary, fig.width=6, fig.height=4, fig.alt="A summary-level point plot with the fraction modified bases per position."}
plotRegion(se_summary, region = "chr1:6932700-6932800",
           tracks.summary = list(FracMod = "Point"))
```

... and here we plot the read-level data of the same region:

```{r plot-region1, warning=FALSE, fig.width=8, fig.height=2.5, fig.alt="A read-level heatmap showing modification probabilities of individual modified bases of each read grouped by sample."}
plotRegion(se, region = "chr1:6932700-6932800",
           tracks.summary = NULL,
           tracks.reads = list(mod_prob = "Heatmap"))
```
```{r plot-region2, warning=FALSE, fig.width=8, fig.height=2.5, fig.alt="A read-level point (lollipop) plot showing modification probabilities of individual modified bases of each read grouped by sample."}
plotRegion(se, region = "chr1:6932700-6932800",
           tracks.summary = NULL,
           tracks.reads = list(mod_prob = "Lollipop"))
```

The x-axis in these plots is in "base-space", meaning that it shows the
coordinates of genomic bases on which modifications can be irregularly spaced.
Alternatively, we can also generate these plot in "modbase-space", in which
only modified bases are shown and the gaps between them are removed:

```{r plot-region-modbasespace1, warning=FALSE, fig.width=8, fig.height=2.5, fig.alt="A read-level heatmap showing modification probabilities of individual modified bases only, without interviening non-modified bases."}
plotRegion(se, region = "chr1:6932700-6932800",
           tracks.summary = NULL,
           tracks.reads = list(mod_prob = "Heatmap"),
           modbaseSpace = TRUE)
```

```{r plot-region-modbasespace2, warning=FALSE, fig.width=8, fig.height=2.5, fig.alt="A read-level point (lollipop) plot showing modification probabilities of individual modified bases only, without interviening non-modified bases."}
plotRegion(se, region = "chr1:6932700-6932800",
           tracks.summary = NULL,
           tracks.reads = list(mod_prob = "Lollipop"),
           modbaseSpace = TRUE)
```

As mentioned above, the read-level data is still contained in the
summary object `se_summary`, so we can plot both summary-level
and read-level data simultaneously with this object as input:

```{r plot-summary-and-read-level, fig.width=8, fig.height=6, fig.alt="A plot of modification data combining different track types."}
plotRegion(se_summary, region = "chr1:6932700-6932800",
           tracks.summary = list(FracMod = c("Smooth")),
           tracks.reads = list(mod_prob = c("Heatmap", "Lollipop")))
```

## Session info

```{r session-info}
sessionInfo()
```

